adaboost
---------------------------
+ 对于提升方法，有两个问题需要回答：
    * 1. 每一轮如何改变训练数据的权值和概率分布
    * 2. 如何将弱分类器组合成一个强分类器
+ Adaboost的做法：
    * 1. 提高被前一轮弱分类器错误分类样本的权值，降低被正确分类的权值
    * 2. 加权多数表决，加大分类误差率小的权重
+ Adaboost算法细节：
    * 具有权值分布$D_m$的训练数据集，得到基本分类器$G_m$
    + 计算$G_m$在训练数据集上的分类误差率 $e_m=\sum_{i=1}^{N}w_{mi}I(G_m(x_i)\neq y_i)$
        * 分类误差率是误样本权值之和
    + 计算$G_m$的系数$\alpha_m=\frac{1}{2}log\frac{1-e_m}{e_m}$
        * $e_m\leq \frac{1}{2}$时，$\alpha_m\geq 0$
        * 误差越小，系数越大
    + 更新训练数据集的权重分布
        * $w_{m+1,i}=\frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i))$
        * $-y_iG_m(x_i)$为正表示误分类
        * $\alpha_m$表示$G_m$的权重
    + 构建基本分类器的线性组合
        * $f(x)=\sum_{m=1}^{M}\alpha_mG_m(x)$
+ Adaboost解释
    * 模型：加法模型
    * 损失函数：指数函数
    * 学习算法：前向分步算法
    * 二分类学习算法
+ 前向分步算法与Adaboost
    * $f_m(x)=f_{m-1}(x)+\alpha_mG_m(x)$
    * 目标是通过前向分步算法得到$M$个$\alpha_m$和$G_m(x)$
    * 指数损失函数：$L(y,f(x))=exp[-yf(x)]$
    * $(\alpha_m,G_m(x))=\arg\min_{\alpha,G}\sum_{i=1}^{N}exp[-y_i(f_{m-1}(x_i)+\alpha G(x_i))]$
    + $(\alpha_m,G_m(x))=\arg\min_{\alpha,G}\sum_{i=1}^{N}w_{mi}exp[-y_i\alpha G(x_i)]$
        * $w_{mi}=exp(-y_if_{m-1}(x_i))$
        * $w_{mi}=exp(-y_i \alpha_{m-1} G_{m-1}(x_i))$
        * $w$既不依赖于$\alpha$也不依赖于$G$，所以与最小化(argmin)无关。$w$依赖于$f$，所以随着每轮迭代而变化
    * 首先求$G_m^*(x)$，$G_m^*(x)=\arg\min_G\sum_{i=1}^{N}w_{mi}I(y_i\neq G(x_i))$
    + 之后求$\alpha_m^*$，对于待优化最小项$\sum_{i=1}^{N}w_{mi}exp[-y_i\alpha G(x_i)]$
        * $=\sum_{y_i=G_m(x_i)}w_{mi}e^{-\alpha}+\sum_{y_i\neq G_m(x_i)}w_{mi}e^{\alpha}$
        * $=(e^a-e^{-a})\sum^N_{i=1} w_{mi}I(y_i\neq G(x_i))+e^{-\alpha}\sum_{i=1}^Nw_{mi}$
    + 将已求得的$G_m^*(x)$带入上式，对$\alpha$求导，解出最小的$\alpha$
        * $\alpha_{m}^{*}=\frac{1}{2} \log \frac{1-e_{m}}{e_{m}}$